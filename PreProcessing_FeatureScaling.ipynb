{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Feature Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exploring about all other aspects, it is essential to know a bit about it. Well, Feature scaling, in the context of machine learning, refers to the process of transforming the numerical features of a dataset into a standardized range. involves bringing all the features to a similar scale, so that no single feature dominates the learning algorithm. By scaling the features, we can ensure that they contribute equally to the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of Feature Scaling in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in machine learning for a variety of reasons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <li>Many machine learning algorithms use distance-based calculations to make predictions. If the features are not scaled, those with larger values can have a disproportionate impact on the results.</li>\n",
    "  <li>Feature scaling can help improve the convergence speed and performance of some optimization algorithms.</li>\n",
    "  <li>This helps in handling skewed data and outliers, which can influence the model’s behavior.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Feature Scaling in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Enhancing Model Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling can significantly enhance the performance of machine learning models. Scaling the features makes it easier for algorithms to find the optimal solution, as the different scales of the features do not influence them. It can lead to faster convergence and more accurate predictions, especially when using algorithms like k-nearest neighbors, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Addressing Skewed Data and Outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewed data and outliers can negatively impact the performance of machine learning models. Scaling the features can help in handling such cases. By transforming the data to a standardized range, it reduces the impact of extreme values and makes the model more robust. This is particularly beneficial for algorithms that assume a normal distribution and are sensitive to outliers, such as linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Faster Convergence During Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradient descent-based algorithms, feature scaling can speed up the convergence by helping the optimization algorithm reach the minima faster. Since gradient descent updates the model parameters in steps proportional to the gradient of the error with respect to the parameter, having features on the same scale allows the algorithm to take more uniform steps towards the optimum and reduces the number of iterations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Balanced Feature Influence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When features are on different scales, there is a risk that larger-scale features will dominate the model’s decisions, while smaller-scale features are neglected. Feature scaling ensures that each feature has the opportunity to influence the model without being overshadowed by other features simply because of their scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Improved Algorithm Behavior:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain machine learning algorithms, particularly those that use distance metrics like Euclidean or Manhattan distance, assume that all features are centered around zero and have variance in the same order. Without feature scaling, the distance calculations could be skewed, leading to biases in the model and potentially misleading results. Feature scaling normalizes the range of features so that each one contributes equally to the distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Feature Scaling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different types of feature scaling techniques as well that you will learn during machine learning free course. These are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Normalization or Min-max scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization, also known as min-max scaling, transforms the features to a range between 0 and 1. It subtracts the minimum value of the feature and divides it by the range (maximum value minus minimum value). This technique is suitable when the distribution of the data does not follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original data: \n",
      " [[ 2.1 -1.9  5.5]\n",
      " [-1.5  2.4  3.5]\n",
      " [ 0.5 -7.9  5.6]\n",
      " [ 5.9  2.3 -5.8]]\n",
      "\n",
      " Min max scaled data: \n",
      " [[0.48648649 0.58252427 0.99122807]\n",
      " [0.         1.         0.81578947]\n",
      " [0.27027027 0.         1.        ]\n",
      " [1.         0.99029126 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Imporing packages\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## Making data\n",
    "input_data = np.array([[2.1, -1.9, 5.5],\n",
    "                      [-1.5, 2.4, 3.5],\n",
    "                     [0.5, -7.9, 5.6],\n",
    "                     [5.9, 2.3, -5.8]])\n",
    "print(\"\\n Original data: \\n\",input_data)\n",
    "\n",
    "## Normalization or Min-max scaling\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaler_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print(\"\\n Min max scaled data: \\n\",data_scaler_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Standardization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization transforms the features to have a mean of 0 and a standard deviation of 1. It subtracts the mean of the feature and divides it by the standard deviation. This technique is preferable when the data is normally distributed or when we don’t know the distribution in advance. Standardization maintains the shape of the distribution and does not bound the features to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original data: \n",
      " [[ 2.1 -1.9  5.5]\n",
      " [-1.5  2.4  3.5]\n",
      " [ 0.5 -7.9  5.6]\n",
      " [ 5.9  2.3 -5.8]]\n",
      "Mean:  [ 1.75  -1.275  2.2  ]\n",
      "Standard (Std) Deviation:  [2.71431391 4.20022321 4.69414529]\n",
      "\n",
      " Standardized data: \n",
      " [[ 0.12894603 -0.14880162  0.70300338]\n",
      " [-1.19735598  0.8749535   0.27694073]\n",
      " [-0.46052153 -1.57729713  0.72430651]\n",
      " [ 1.52893149  0.85114524 -1.70425062]]\n",
      "Mean:  [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Standard (Std) Deviation:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Imporing packages\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## Making data\n",
    "input_data = np.array([[2.1, -1.9, 5.5],\n",
    "                      [-1.5, 2.4, 3.5],\n",
    "                     [0.5, -7.9, 5.6],\n",
    "                     [5.9, 2.3, -5.8]])\n",
    "print(\"\\n Original data: \\n\",input_data)\n",
    "\n",
    "## Standardization\n",
    "print(\"Mean: \", input_data.mean(axis=0))\n",
    "print(\"Standard (Std) Deviation: \", input_data.std(axis=0))\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\n Standardized data: \\n\",data_scaled)\n",
    "print(\"Mean: \", data_scaled.mean(axis=0))\n",
    "print(\"Standard (Std) Deviation: \", data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1, l2, or max norms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original data: \n",
      " [[ 2.1 -1.9  5.5]\n",
      " [-1.5  2.4  3.5]\n",
      " [ 0.5 -7.9  5.6]\n",
      " [ 5.9  2.3 -5.8]]\n",
      "\n",
      " l1 normalized data: \n",
      " [[ 0.22105263 -0.2         0.57894737]\n",
      " [-0.2027027   0.32432432  0.47297297]\n",
      " [ 0.03571429 -0.56428571  0.4       ]\n",
      " [ 0.42142857  0.16428571 -0.41428571]]\n",
      "\n",
      " l2 normalized data: \n",
      " [[ 0.33946114 -0.30713151  0.88906489]\n",
      " [-0.33325106  0.53320169  0.7775858 ]\n",
      " [ 0.05156558 -0.81473612  0.57753446]\n",
      " [ 0.68706914  0.26784051 -0.6754239 ]]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Imporing packages\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## Making data\n",
    "input_data = np.array([[2.1, -1.9, 5.5],\n",
    "                      [-1.5, 2.4, 3.5],\n",
    "                     [0.5, -7.9, 5.6],\n",
    "                     [5.9, 2.3, -5.8]])\n",
    "print(\"\\n Original data: \\n\",input_data)\n",
    "\n",
    "## Normalizing\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm=\"l1\")\n",
    "print(\"\\n l1 normalized data: \\n\",data_normalized_l1)\n",
    "# least square\n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm=\"l2\")\n",
    "print(\"\\n l2 normalized data: \\n\",data_normalized_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Examples and Best Practices for Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding feature scaling in theory is one thing, but applying it correctly in practice is equally important. Consider the following best practices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Feature Scaling Workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying feature scaling, it’s crucial to fit the scaler on the training data and then use the same scaler to transform the test data. This ensures consistency and prevents data leakage. Furthermore, it’s beneficial to evaluate the impact of feature scaling on your specific machine learning task to determine which method works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Handling Categorical Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is not typically applied to categorical variables, as their values represent different categories and do not have a numerical scale. Categorical variables may require additional preprocessing techniques such as one-hot encoding before being used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Dealing with Time-Series Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time-series data, feature scaling can be applied to each individual time series or across all time series, depending on the context. Consider the characteristics of your data and the requirements of your machine learning model to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in machine learning by ensuring that all features are on a similar scale, preventing bias towards certain features. It improves the performance and convergence of many machine learning algorithms. As the field of machine learning continues to evolve, new feature scaling methods and techniques may emerge. It’s important to stay updated and choose the most appropriate feature scaling method for each specific task. Remember, a well-scaled feature is a step closer to a well-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
